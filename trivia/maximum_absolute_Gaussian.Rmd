---
title: "Maximum Absolute Value of i.i.d. Gaussian Random Variables"
output: html_notebook
---

This is directly copied from [this post](https://math.stackexchange.com/questions/1456567/expected-maximum-absolute-value-of-n-iid-standard-gaussians) by [zhoraster](http://probability.univ.kiev.ua/index.php?page=userinfo&person=zhoraster), just in case the page gets removed.

Let $X_1, X_2, \cdots, X_n$ be independent standard Gaussian random variables. Then $$\mathbb{E}\left[\max_{i=1}^n \left|X_i\right| \right] \leq \sqrt{C\log_e n}.$$

The inequality holds with $C=2$, and this is the optimal constant (the optimality follows from [here](https://math.stackexchange.com/questions/89030/expectation-of-the-maximum-of-gaussian-random-variables)).

Outline of the proof: 

1. Let $\varphi$ and $\Psi$ be the pdf and complementary cdf of standard normal distribution. Using the inequality $\Psi(x)< \varphi(x)/x$ for $x>0$, it is easy to show that $-\log \Psi(x)$ is convex, therefore, its inverse $G(t) = \Psi^{-1}(e^{-t})$ is concave. Note also that $G$ is increasing.

2. Applying the quantile transformation, $X_k = G(Y_k)$, where $Y_k$ are iid $\operatorname{Exp}(1)$. Denoting $X_{(n)} = \max_k X_k$, $Y_{(n)} = \max_k Y_k$ and using the monotonicity and concavity of $G$, we get with the help of Jensen's inequality
$$
E[X_{(n)}] = E[G(E_{(n)})]\le G(E[Y_{(n)}]) = G(H_n),
$$
where $H_n = 1+\frac12 + \dots + \frac1n$ is the $n$th harmonic number (the distribution of exponential order statistics is well known). Since $H_n\le \log n + 1$ for all $n$, we get
$$
E[X_{(n)}]\le G(\log n+1) = \Psi^{-1}\big(\tfrac1{en}\big).
$$

3. Using the inequality $\Psi(x)< \varphi(x)/x$ again, 
$$
\Psi(\sqrt{2 \log n}) \le \frac{1}{2\sqrt{\pi\log n}}e^{-\log n} \le \frac{1}{en}.
$$
As $\Psi$ decreases, 
$$
E[X_{(n)}]\le \sqrt{2\log n},
$$
as required.