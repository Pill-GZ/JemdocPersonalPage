<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<link rel="stylesheet" href="../jemdoc.css" type="text/css" />
<title>Some statistics related things</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Info&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</div>
<div class="menu-item"><a href="../index.html">Main</a></div>
<div class="menu-category">Research</div>
<div class="menu-item"><a href="../research/index.html">Papers</a></div>
<div class="menu-item"><a href="../trivia/index.html" class="current">Trivia</a></div>
<div class="menu-item"><a href="../software/index.html">Software</a></div>
<div class="menu-category">Others</div>
<div class="menu-item"><a href="../consulting/index.html">Consulting</a></div>
<div class="menu-item"><a href="../teaching/index.html">Teaching</a></div>
<div class="menu-item"><a href="../personal/index.html">Personal</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Some statistics related things</h1>
<div id="subtitle">collected here to share and for reference</div>
</div>
<h2>R &amp; Shiny</h2>
<ul>
<li><p>A <a href="http://mathesaurus.sourceforge.net/r-numpy.html">list of simliar functions in numpy for R users</a>.</p>
</li>
</ul>
<ul>
<li><p>Add a load spinner in Shiny with <a href="https://github.com/andrewsali/shinycssloaders">shinycssloaders</a> in two lines of code. <a href="https://cran.r-project.org/web/packages/shinycssloaders/index.html">Package on CRAN</a> (Mar 2019)</p>
</li>
</ul>
<ul>
<li><p>Your don't have to know JavaScript to use JavaScript libraries in Shiny. A guide to <a href="http://shiny.rstudio.com/articles/js-build-widget.html">htmlwidget</a>. (Mar 2019)</p>
</li>
</ul>
<ul>
<li><p>To use output values in a conditionalPanel, you must render it in UI first. See <a href="https://stackoverflow.com/questions/21609436/r-shiny-conditionalpanel-output-value">this post</a>. (Feb 2019)</p>
</li>
</ul>
<ul>
<li><p>Caching plots in Shiny <a href="https://blog.rstudio.com/2018/11/13/shiny-1-2-0/">is now possible</a> (though not with Plotly yet). (Feb 2019)</p>
</li>
</ul>
<ul>
<li><p>A post on <a href="https://stackoverflow.com/questions/39436713/r-shiny-reactivevalues-vs-reactive">the difference between reactive and reactiveValues</a> in Shiny (Feb 2019).</p>
</li>
</ul>
<ul>
<li><p><a href="https://github.com/yihui/printr">printr (prints help pages with knitr)</a></p>
</li>
</ul>
<ul>
<li><p><a href="https://www.stat.ubc.ca/~jenny/STAT545A/topic10_tablesCSS.html">Fixing the annoying Rmarkdown table width default</a></p>
</li>
</ul>
<h2>Stats</h2>
<ul>
<li><p>CV errors of selected models are biased, for the same reason that naive post-selection inference/estimation of effect sizes are. See <a href="https://link.springer.com/article/10.1186/1471-2105-7-91">Varma &amp; Simon (2006)</a>, <a href="http://www.jmlr.org/papers/v11/cawley10a.html">Cawley &amp; Talbot (2010)</a>, and <a href="https://www.sciencedirect.com/science/article/pii/S0020025511006773">Bergmeir &amp; Benítez (2012)</a> (Feb 2020).</p>
</li>
</ul>
<ul>
<li><p>The OLS solution can be written as a weighted average of all n-choose-(p+1) slopes defined by all subsets of (p+1) data points that form a plane. See <a href="https://academic.oup.com/biomet/article/75/4/779/423143">a theorem of Jacobi and its generalization
by Mark Berman (1988)</a>. (I got it from <a href="http://faculty.washington.edu/kenrice/index.html">Ken Rice</a>'s BIOST 571 notes, Nov 2019)</p>
</li>
</ul>
<ul>
<li><p>The ridge solution can be written as a weighted average of 2^p regression coefficients on all possible subsets of variables. See <a href="https://www.jstor.org/stable/2984832?seq=1">Leamer and Chaimberlain (1976)</a>. (Nov 2019)</p>
</li>
</ul>
<ul>
<li><p>A <a href="./prob_puzzle2.html">brainteaser</a> that I couldn't solve during an interview. Not really stats-related. (Oct 2019)</p>
</li>
</ul>
<ul>
<li><p>Don't use <a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic">AUROC</a> when evaluating the predictive performance for rare events. The <a href="https://en.wikipedia.org/wiki/Precision_and_recall">precision-recall curve</a> handles multiple testing (through FDR) and better reflects the difficulty of the problem. 
See also, a <a href="https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/">post</a> by <a href="https://machinelearningmastery.com/about/">Jason Brownlee</a>. (Jul 2019)</p>
</li>
</ul>
<ul>
<li><p><a href="https://timvieira.github.io/blog/post/2014/08/01/gumbel-max-trick-and-weighted-reservoir-sampling/">A sampling trick on data streams</a>. (May 2019)</p>
</li>
</ul>
<ul>
<li><p>A <a href="./prob_puzzle.nb.html">probability puzzle</a> that I couldn't solve during an interview. (Dec 2018)</p>
</li>
</ul>
<ul>
<li><p>My second favorite statistical head-scratcher (after <a href="https://en.wikipedia.org/wiki/Stein%27s_example">Stein's paradox</a>) is <a href="./thresholding_paradox.nb.html">the sub-optimality of thresholding procedures</a> (partly because we found it). (Nov 2018)</p>
</li>
</ul>
<ul>
<li><p>There is (&ldquo;as there ought to be&rdquo;) a <a href="https://stats.stackexchange.com/questions/81395/relationship-between-ridge-regression-and-pca-regression">connection between ridge regression and principal component regression</a>. See also, the <a href="https://www.tandfonline.com/doi/abs/10.1080/01621459.1977.10479910">&ldquo;57 varieties&rdquo; paper</a> by Dempster et al..</p>
</li>
</ul>
<ul>
<li><p><a href="https://terrytao.wordpress.com/category/teaching/254a-random-matrices/">Terence Tao's notes on random matrices</a></p>
</li>
</ul>
<ul>
<li><p>The Alternating Conditional Expectation algorithm proposed in <a href="https://pdfs.semanticscholar.org/e1ee/012f12793b4021352bb953f2fe9a40c33cf2.pdf">Breiman and Friedman (1985)</a>. See a quick <a href="./ACE.nb.html">replicate of the first example</a>. I am surprised that I have never heard of it until now. (Feb 2018)</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/1405.0680.pdf">A useful variant of the Davis–Kahan theorem for statisticians</a> which bounds the distances between subspaces spaned by eigenvectors of a perturbed covariance matrix and the its counterpart of the uncontaminated one. Related is a <a href="./subspace_dist.html">variational characterization of distances between subspaces</a>.</p>
</li>
</ul>
<ul>
<li><p>An iterative method for <a href="./steifel_notes.pdf">Optimization on Stiefel manifolds</a> by <a href="http://noodle.med.yale.edu/hdtag/notes.html">Hemant D. Tagare</a>. This method happens to fail when optimizating tr(XX'S) over X in the Steifel manifold when S is real symmetric (which is a common problem is statistics). See this <a href="https://arxiv.org/pdf/1511.03688.pdf">review paper</a> for a comparison of procedures for eigen-problems on data streams. One of the ideas is based on <a href="http://link.springer.com/article/10.1007/BF01396012">rank-one modification of the symmetric eigen-problem</a>.</p>
</li>
</ul>
<ul>
<li><p><a href="./tree_distance_metric.html">Some thoughts on trees and distance metrics</a>. You can recover the tree structure from the distance matrix of its nodes. (see <a href="./tree_distance_matrix.html">an R script</a>)</p>
</li>
</ul>
<ul>
<li><p>You cannot arbitrarily design full conditional distributions and expect them to be compatible (only when they are compatible can you use Brook's lemma). I was suprised how this is glossed over by many people. (<a href="./incompatible_full_conditionals.html">numerical example</a>, <a href="https://www.jstor.org/stable/2289858">theory</a>)</p>
</li>
</ul>
<ul>
<li><p>There is little reason to unconditionally favor <a href="https://en.wikipedia.org/wiki/Fisher's_method">Fisher's method</a> (sum of negative log p-values) over, say, Edginton's Method (sum of p-values) for combining p-values (although it is better known that Fisher's method and Bonferroni's method are good at picking out different alternatives). In some cases of small and distributed effects, the latter wins; see <a href="./combine_p-values.html">numerical example</a>, <a href="./Liptak_1958.pdf">theory</a>. In particular, Edginton's Method is optimal when the p-values under the alternative are truncated exponentials.</p>
</li>
</ul>
<ul>
<li><p>Don't include both log(x) and log^2(x) in your linear model; see <a href="https://stats.stackexchange.com/questions/312662/multicollinearity-between-lnx-and-lnx2">this post</a>.</p>
</li>
</ul>
<div id="footer">
<div id="footer-text">
Page generated 2020-06-20 17:35:43 CDT, by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
